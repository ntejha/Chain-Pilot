# Chainâ€‘Pilot

A Gradioâ€‘based chatbot that integrates an Ollama LLM with two FastAPIâ€‘MCP tool servers (News and Weather) to fetch realâ€‘time data on demand.

---

## ğŸ“ Project Structure

```
Chain-Pilot/
â”œâ”€â”€ app.py                # Main Gradio+OllamaMCPOAdapter chat application
â”œâ”€â”€ mcp-server/
â”‚   â”œâ”€â”€ news_api.py       # FastAPI-MCP server for news tool (portÂ 8023)
â”‚   â””â”€â”€ weather_api.py    # FastAPI-MCP server for weather tool (portÂ 8024)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # This file
```

---

## âš™ï¸ Prerequisites

1. **PythonÂ 3.10+**
2. **Ollama** installed and running locally (HTTP API on `localhost:11434`).
3. **Git** (to clone this repo).

---

## ğŸ› ï¸ Setup & Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-org/Chain-Pilot.git
   cd Chain-Pilot
   ```

2. **Create and activate a virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate    # Linux/macOS
   venv\Scripts\activate     # Windows
   ```

3. **Install Python dependencies**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Pull your Ollama model**
   ```bash
   # Example: llama2:13b or deepseek:8b
   ollama pull llama2:13b
   ```

5. **(Optional)** Edit `app.py` to change the model or MCP ports:
   ```diff
   - MODEL_NAME = "llama2:13b"
   + MODEL_NAME = "deepseek:8b"

   - NEWS_MCP = {"host": "localhost", "port": 8023}
   + NEWS_MCP = {"host": "127.0.0.1", "port": 8023}
   ```

---

## ğŸš€ Running the MCP Servers

Open two separate terminals (with your virtual environment activated) and start each service:

```bash
# TerminalÂ 1 â€“ News API
python mcp-server/news_api.py

# TerminalÂ 2 â€“ Weather API
python mcp-server/weather_api.py
```

Each will print FastAPI/Uvicorn startup logs and begin listening on portsÂ 8023 andÂ 8024 respectively.

---

## â–¶ï¸ Launching the Chat App

With the MCP servers running, start the Gradio interface:

```bash
python app.py
```

Gradio will print a local URL, e.g.:  
```
Running on http://127.0.0.1:7860
```

Open that URL in your browser to interact:

- Type questions like **â€œWhat are todayâ€™s top headlines?â€** or **â€œWhatâ€™s the weather in London?â€**
- The Ollama model will invoke the News and Weather MCP tools as needed and display live data.

---

## ğŸŒ Environment Variables

If your Ollama API host or port differs, set the `OLLAMA_HOST` environment variable before running:

```bash
export OLLAMA_HOST=http://localhost:11434
```  
(Or adjust it at the top of `app.py`.)

---

## ğŸ§© Customization

- **Switch LLM model**: Edit `MODEL_NAME` at the top of `app.py` and rerun `ollama pull` if needed.
- **Add more MCP tools**: Create new FastAPI-MCP services, then update `app.py` to include a new `OllamaMCPOAdapter` and merge its `list_tools_ollama()` into `ALL_TOOLS`.

---

## ğŸ“ License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.

